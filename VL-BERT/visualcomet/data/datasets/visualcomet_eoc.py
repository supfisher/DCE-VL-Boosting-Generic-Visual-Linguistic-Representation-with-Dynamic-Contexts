"""
    In this file, we generate positive image paris according to the event-based similarity.
    The pytorch dataset is called ImgLoader to generate each image item.
    Generated by Guoqing Ma in KAUST.
"""

"""
visualcomet dataset record sample:
{'img_fn': 'movieclips_American_Psycho/MCo6TtUkCWc@16.jpg', 
'movie': 'American Psycho (2000)', 
'metadata_fn': 'movieclips_American_Psycho/MCo6TtUkCWc@16.json', 
'split': 'train', 
'place': 'a crowded bar', 
'event': '2 sits in a chair turning towards the people behind him', 
'event_idx': 0, 
'inference_relation': 'intent', 
'inference_text': 'have a drink at the bar after work', 
'event_name': '<|det2|> sits in a chair turning towards the people behind him', 
'inference_text_name': 'have a drink at the bar after work', 
'person2name': {'2': '<|det2|>'}, 
'name2person': {'<|det2|>': '2'}}
"""

import json
from PIL import Image
import logging
from torch.utils.data import Dataset
import torch
import math
import os
from tqdm import tqdm
import numpy as np
import random
from torch.nn import functional as F
import sys
sys.path.append("/ibex/scratch/mag0a/Github/VL-BERT/")
from external.pytorch_pretrained_bert import BertTokenizer
from common.utils.create_logger import makedirsExist
from visualcomet.data.transforms import transforms as T

import sys
sys.path.append("/ibex/scratch/mag0a/Github/visual-comet/")
from utils.file_utils import read_and_parse_finetune_json


VCR_IMAGES_DIR = '/ibex/scratch/mag0a/Github/visual-comet/data/vcr1images'# os.environ['VCR_PARENT_DIR']
VCR_FEATURES_DIR = '/ibex/scratch/mag0a/Github/visual-comet/data/features'
record_cache = 'pretrain/data/visualcomet_preparetion/cache/event_similarity_norm/'


class VisualCometDatasetEoc(Dataset):
    def __init__(self, root_path='./', seq_len=64,
                 with_precomputed_visual_feat=False, mask_raw_pixels=True,
                 with_rel_task=True, with_mlm_task=True, with_mvrc_task=True,
                 split='train',
                 transform=None, test_mode=False,
                 zip_mode=False, cache_mode=False, cache_db=False, ignore_db_cache=True,
                 tokenizer=None, pretrained_model_name=None,
                 add_image_as_a_box=False,
                 aspect_grouping=False,
                 vcg_dir='/ibex/scratch/mag0a/Github/visual-comet/data/visualcomet_annotations',
                 **kwargs):
        """
        VisualCometDataset Image Dataset
        """
        super(VisualCometDatasetEoc, self).__init__()
        self.vcg_dir = vcg_dir
        self.seq_len = seq_len
        self.with_rel_task = with_rel_task
        self.with_mlm_task = with_mlm_task
        self.with_mvrc_task = with_mvrc_task
        self.split = split
        self.data_path = None
        self.root_path = root_path
        self.with_precomputed_visual_feat = with_precomputed_visual_feat
        self.mask_raw_pixels = mask_raw_pixels
        self.image_set = None
        self.test_mode = test_mode
        self.zip_mode = zip_mode
        self.cache_mode = cache_mode
        self.cache_db = cache_db
        self.ignore_db_cache = ignore_db_cache
        self.aspect_grouping = aspect_grouping
        self.add_image_as_a_box = add_image_as_a_box

        self.transform = transform
        self.cache_dir = os.path.join(self.root_path, 'cache')
        if not os.path.exists(self.cache_dir):
            makedirsExist(self.cache_dir)

        self.tokenizer = BertTokenizer.from_pretrained(
            'bert-base-cased', do_lower_case='uncased' in 'bert-base-cased')

        self.records = self.load_records()
        self.len_records = len(self.records)
        self.event_label = {'before': 1, 'intent': 2, 'after': 3 }

        with open(os.path.join(os.path.dirname(__file__), 'cocoontology.json'), 'r') as f:
            coco = json.load(f)
        self.coco_objects = ['__background__'] + [x['name'] for k, x in sorted(coco.items(), key=lambda x: int(x[0]))]
        self.coco_obj_to_ind = {o: i for i, o in enumerate(self.coco_objects)}


    @property
    def data_names(self):
        return ['image', 'boxes', 'im_info', 'text', 'relationship_label']

    def __getitem__(self, index):
        return self.load_imgs(index)

    def load_records(self):
        split_filename = '{}_annots.json'.format(self.split)
        vcg_path = os.path.join(self.vcg_dir, split_filename)
        print("Loading vcg dataset {}".format(vcg_path))

        records = read_and_parse_finetune_json(vcg_path)
        return records

    def __len__(self):
        return len(self.records)*2

    def _load_image(self, path):
        return Image.open(path).convert('RGB')

    def _load_json(self, path):
        with open(path, 'r') as f:
            return json.load(f)

    def load_imgs(self, index):
        record = self.records[index % self.len_records]
        img_fn, metadata_fn = record['img_fn'], record['metadata_fn']

        if index < self.len_records:
            relationship_labels = self.event_label[record['inference_relation']]
        else:
            neg_index = random.randint(0, self.len_records-1)
            while record['img_fn'] == self.records[neg_index % self.len_records]['img_fn']:
                neg_index = random.randint(0, self.len_records - 1)
            record = self.records[neg_index]
            relationship_labels = 0

        if relationship_labels in [0, 2]: # negative label or intent
            caption = record['event']
        else:
            caption = record['inference_text']

        caption_tokens = self.tokenizer.tokenize(caption)
        text_tokens = ['[CLS]'] + caption_tokens + ['[SEP]']
        text = self.tokenizer.convert_tokens_to_ids(text_tokens)

        # Load boxes and their features.
        with open(os.path.join(VCR_IMAGES_DIR, metadata_fn), 'r') as f:
            metadata = json.load(f)

        boxes = torch.as_tensor(np.array(metadata['boxes']))[:, :-1]

        objects = metadata['names']
        obj_labels = [self.coco_obj_to_ind[obj] for obj in objects]

        boxes_cls_scores = boxes.new_zeros((boxes.shape[0], 81))
        for i, class_ in enumerate(obj_labels):
            boxes_cls_scores[i, class_] = 1.0

        image = self._load_image(os.path.join(VCR_IMAGES_DIR, img_fn))
        w0, h0 = image.size

        if self.add_image_as_a_box:
            image_box = torch.as_tensor([[0.0, 0.0, w0 - 1.0, h0 - 1.0]])
            boxes = torch.cat((image_box, boxes), dim=0)
            obj_labels = [self.coco_obj_to_ind['__background__']] + obj_labels
            if self.with_precomputed_visual_feat:
                image_box_feat = boxes.mean(dim=0, keepdim=True)
                boxes = torch.cat((image_box_feat, boxes), dim=0)

        # transform
        im_info = torch.tensor([w0, h0, 1.0, 1.0, index])
        if self.transform is not None:
            image, boxes, _, im_info = self.transform(image, boxes, None, im_info)

        # clamp boxes
        w = im_info[0].item()
        h = im_info[1].item()
        boxes[:, [0, 2]] = boxes[:, [0, 2]].clamp(min=0, max=w - 1)
        boxes[:, [1, 3]] = boxes[:, [1, 3]].clamp(min=0, max=h - 1)

        # truncate seq to max len
        if len(text) + len(boxes) > self.seq_len:
            text_len_keep = len(text)
            box_len_keep = len(boxes)
            while (text_len_keep + box_len_keep) > self.seq_len:
                if box_len_keep > text_len_keep:
                    box_len_keep -= 1
                else:
                    text_len_keep -= 1
            boxes = boxes[:box_len_keep]
            text = text[:text_len_keep]

        return image, boxes, im_info, text, relationship_labels