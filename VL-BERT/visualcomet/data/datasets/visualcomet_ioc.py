"""
    In this file, we generate positive image paris according to the event-based similarity.
    The pytorch dataset is called ImgLoader to generate each image item.
    Generated by Guoqing Ma in KAUST.
"""

"""
visualcomet dataset record sample:
{'img_fn': 'movieclips_American_Psycho/MCo6TtUkCWc@16.jpg', 
'movie': 'American Psycho (2000)', 
'metadata_fn': 'movieclips_American_Psycho/MCo6TtUkCWc@16.json', 
'split': 'train', 
'place': 'a crowded bar', 
'event': '2 sits in a chair turning towards the people behind him', 
'event_idx': 0, 
'inference_relation': 'intent', 
'inference_text': 'have a drink at the bar after work', 
'event_name': '<|det2|> sits in a chair turning towards the people behind him', 
'inference_text_name': 'have a drink at the bar after work', 
'person2name': {'2': '<|det2|>'}, 
'name2person': {'<|det2|>': '2'}}
"""

import json
from PIL import Image
import logging
from torch.utils.data import Dataset
import torch
import math
import os
from tqdm import tqdm
import numpy as np
import random
from torch.nn import functional as F
import sys
sys.path.append("/ibex/scratch/mag0a/Github/VL-BERT/")
from external.pytorch_pretrained_bert import BertTokenizer
from common.utils.create_logger import makedirsExist
from visualcomet.data.transforms import transforms as T

import sys
sys.path.append("/ibex/scratch/mag0a/Github/visual-comet/")
from utils.file_utils import read_and_parse_finetune_json


VCR_IMAGES_DIR = '/ibex/scratch/mag0a/Github/visual-comet/data/vcr1images'# os.environ['VCR_PARENT_DIR']
VCR_FEATURES_DIR = '/ibex/scratch/mag0a/Github/visual-comet/data/features'
record_cache = 'pretrain/data/visualcomet_preparetion/cache/event_similarity_norm/'


def convert_str_mins(time):
    time_list = time.split('.')
    assert len(time_list) == 4
    time = int(time_list[-1]) + int(time_list[-2]) * 60 + int(time_list[-3]) * 3600
    return time


def parse_file_name(file_name):
    ori_file_name = file_name
    file_name = file_name.split('@')
    at_index = file_name[-1].split('.')[0]
    file_name = file_name[0].split('-')
    time_end = file_name[-1]
    file_name = file_name[-2]
    time_begin = file_name[-len("00.17.26.000"):]
    try:
        return convert_str_mins(time_begin), convert_str_mins(time_end), at_index
    except:
        print(ori_file_name)


def is_in_one_cluster(file_prev, file):
    prev_time_begin, prev_time_end, prev_at_index = parse_file_name(file_prev)
    time_begin, time_end, at_index = parse_file_name(file)
    if time_begin == prev_time_begin:
        return True
    else:
        return False


def sample_one_cluster(one_cluster):
    samples = []
    one_cluster = sorted(one_cluster, key=lambda x: int(x.split('@')[1].split('.')[0]))
    if len(one_cluster) > 1:
        for i, img in enumerate(one_cluster):
            for neg_img in one_cluster[:i]:
                samples.append([img, neg_img, 0])
            for pos_img in one_cluster[i + 1:]:
                samples.append([img, pos_img, 1])
    return samples


def gen_image_clusters(path):
    image_clusters = []
    g = os.walk(path)
    for path, dir_list, file_list in g:
        for dir_name in dir_list:
            files = [os.path.join(dir_name, f) for f in os.listdir(os.path.join(path, dir_name)) if
                     ('jpg' in f) and ('npy' not in f)]
            files = sorted(files)
            file_prev = files[0]
            one_cluster = [file_prev]
            if 'lsmdc' in dir_name:  # only lsmdc dir has time stamp
                for file in files[1:]:
                    if is_in_one_cluster(file_prev, file):
                        one_cluster.append(file)
                    else:
                        image_clusters.append(one_cluster)
                        one_cluster = [file]
                    file_prev = file
            else:
                for file in files[1:]:
                    if file_prev.split('@')[0] == file.split('@')[0]:
                        one_cluster.append(file)
                    else:
                        image_clusters.append(one_cluster)
                        one_cluster = [file]
                    file_prev = file

    image_clusters = [one_cluster for one_cluster in image_clusters if len(one_cluster) >= 2]
    return image_clusters


def gen_image_ordering_cache(image_clusters):
    image_ordering_cache = []
    for one_cluster in image_clusters:
        samples = sample_one_cluster(one_cluster)
        image_ordering_cache.extend(samples)
    return image_ordering_cache


class VisualCometDatasetIoc(Dataset):
    def __init__(self, root_path='./', seq_len=64,
                 with_precomputed_visual_feat=False, mask_raw_pixels=True,
                 with_rel_task=True, with_mlm_task=True, with_mvrc_task=True,
                 split='train',
                 transform=None, test_mode=False,
                 zip_mode=False, cache_mode=False, cache_db=False, ignore_db_cache=True,
                 tokenizer=None, pretrained_model_name=None,
                 add_image_as_a_box=False,
                 aspect_grouping=False,
                 vcg_dir='/ibex/scratch/mag0a/Github/visual-comet/data/visualcomet_annotations',
                 **kwargs):
        """
        VisualCometDataset Image Dataset
        """
        super(VisualCometDatasetIoc, self).__init__()
        self.vcg_dir = vcg_dir
        self.seq_len = seq_len
        self.with_rel_task = with_rel_task
        self.with_mlm_task = with_mlm_task
        self.with_mvrc_task = with_mvrc_task
        self.split = split
        self.data_path = None
        self.root_path = root_path
        self.with_precomputed_visual_feat = with_precomputed_visual_feat
        self.mask_raw_pixels = mask_raw_pixels
        self.image_set = None
        self.test_mode = test_mode
        self.zip_mode = zip_mode
        self.cache_mode = cache_mode
        self.cache_db = cache_db
        self.ignore_db_cache = ignore_db_cache
        self.aspect_grouping = aspect_grouping
        self.add_image_as_a_box = add_image_as_a_box

        self.transform = transform
        self.cache_dir = os.path.join(self.root_path, 'cache')
        if not os.path.exists(self.cache_dir):
            makedirsExist(self.cache_dir)

        self.tokenizer = BertTokenizer.from_pretrained(
            'bert-base-cased', do_lower_case='uncased' in 'bert-base-cased')

        self.records = self.load_records()

        ## process image records
        self.img_names = self.img_fns(self.records)
        self.imgfn_index = self.mapping_fns_to_index(self.records)
        path = "/ibex/scratch/mag0a/Github/visual-comet/data/vcr1images"
        if os.path.exists('image_cluster.json'):
            with open('image_cluster.json', 'r') as f:
                data = json.load(f)
                self.image_ordering_one_cluster = data['cluster']
                self.image_ordering_cache = data['cache']
        else:
            image_clusters = gen_image_clusters(path)
            self.image_ordering_one_cluster = []
            self.image_ordering_cache = []
            left_images_len = 0

            for one_cluster in image_clusters:
                samples = sample_one_cluster(one_cluster)
                new_samples = []
                for img_i, img_j, ordering_label in samples:
                    if img_i in self.img_names and img_j in self.img_names:
                        new_samples.append([img_i, img_j, ordering_label])
                if len(new_samples) > 2:
                    self.image_ordering_one_cluster.append(new_samples)
                    self.image_ordering_cache.extend(sorted(new_samples)[:len(new_samples) // 2])
                    left_images_len += len(new_samples) // 2
            with open('image_cluster.json', 'w') as f:
                json.dump({'cache': self.image_ordering_cache,
                           'cluster': self.image_ordering_one_cluster}, f)

        print()

    @property
    def data_names(self):
        return ['image', 'boxes', 'im_info', 'text']

    def __getitem__(self, index):
        id_i, id_j, ordering_label = self.image_ordering_cache[index]
        return self.load_img(self.imgfn_index[id_i]) + self.load_img(self.imgfn_index[id_j]) + (ordering_label,)

    def get_one_cluster(self, samples):
        def yied_one(samples):
            for id_i, id_j, ordering_label in samples:
                yield self.load_img(self.imgfn_index[id_i]) + self.load_img(self.imgfn_index[id_j]) + (ordering_label,)

        ordering = sorted(set(list(zip(*samples))[0]), key=lambda x: int(x.split('@')[1].split('.')[0]))

        return list(yied_one(samples)), ordering

    def load_records(self):
        split_filename = '{}_annots.json'.format('train')
        vcg_path = os.path.join(self.vcg_dir, split_filename)
        print("Loading vcg dataset {}".format(vcg_path))

        # records = read_and_parse_finetune_json(vcg_path)
        with open(vcg_path, 'rb') as f:
            records = json.load(f)
        return records

    def img_fns(self, records):
        img_names = []
        for record in records:
            img_names.append(record['img_fn'])
        return list(set(img_names))

    def mapping_fns_to_index(self, records):
        mappings = {}
        for i, record in enumerate(records):
            img_fn = record['img_fn']
            mappings[img_fn] = i
        return mappings

    def __len__(self):
        return len(self.image_ordering_cache)

    def _load_image(self, path):
        return Image.open(path).convert('RGB')

    def _load_json(self, path):
        with open(path, 'r') as f:
            return json.load(f)

    def load_img(self, index):
        record = self.records[index]
        img_fn, metadata_fn = record['img_fn'], record['metadata_fn']

        caption = record['event']
        caption_tokens = self.tokenizer.tokenize(caption)
        text_tokens = ['[CLS]'] + caption_tokens + ['[SEP]']
        text = self.tokenizer.convert_tokens_to_ids(text_tokens)

        # Load boxes and their features.
        with open(os.path.join(VCR_IMAGES_DIR, metadata_fn), 'r') as f:
            metadata = json.load(f)

        boxes = torch.as_tensor(np.array(metadata['boxes']))[:, :-1]

        image = self._load_image(os.path.join(VCR_IMAGES_DIR, img_fn))
        w0, h0 = image.size

        if self.add_image_as_a_box:
            image_box = torch.as_tensor([[0.0, 0.0, w0 - 1.0, h0 - 1.0]])
            boxes = torch.cat((image_box, boxes), dim=0)
            if self.with_precomputed_visual_feat:
                image_box_feat = boxes.mean(dim=0, keepdim=True)
                boxes = torch.cat((image_box_feat, boxes), dim=0)

        # transform
        im_info = torch.tensor([w0, h0, 1.0, 1.0, index])
        if self.transform is not None:
            image, boxes, _, im_info = self.transform(image, boxes, None, im_info)

        # clamp boxes
        w = im_info[0].item()
        h = im_info[1].item()
        boxes[:, [0, 2]] = boxes[:, [0, 2]].clamp(min=0, max=w - 1)
        boxes[:, [1, 3]] = boxes[:, [1, 3]].clamp(min=0, max=h - 1)

        # truncate seq to max len
        if len(text) + len(boxes) > self.seq_len:
            text_len_keep = len(text)
            box_len_keep = len(boxes)
            while (text_len_keep + box_len_keep) > self.seq_len:
                if box_len_keep > text_len_keep:
                    box_len_keep -= 1
                else:
                    text_len_keep -= 1
            boxes = boxes[:box_len_keep]
            text = text[:text_len_keep]

        return image, boxes, im_info, text


