"""
    In this file, we generate positive image paris according to the event-based similarity.
    The pytorch dataset is called ImgLoader to generate each image item.
    Generated by Guoqing Ma in KAUST.
"""
import json
from PIL import Image
import logging
from torch.utils.data import Dataset
import torch
import math
import os
from tqdm import tqdm
import numpy as np
import random
from torch.nn import functional as F
import sys
sys.path.append("/ibex/scratch/mag0a/Github/VL-BERT/")
from external.pytorch_pretrained_bert import BertTokenizer
from common.utils.create_logger import makedirsExist
from pretrain.data.transforms import transforms as T

import sys
sys.path.append("/ibex/scratch/mag0a/Github/visual-comet/")
import random

VCR_IMAGES_DIR = '/ibex/scratch/mag0a/Github/visual-comet/data/vcr1images'# os.environ['VCR_PARENT_DIR']
VCR_FEATURES_DIR = '/ibex/scratch/mag0a/Github/visual-comet/data/features'
record_cache = '/ibex/scratch/mag0a/Github/VL-BERT/pretrain/data/visualcomet_preparetion/cache/event_similarity_cleaned'


class VisualCometDataset(Dataset):
    def __init__(self, root_path='./', seq_len=64,
                 with_precomputed_visual_feat=False, mask_raw_pixels=True,
                 with_rel_task=True, with_mlm_task=True, with_mvrc_task=True,
                 with_contrastive_task=True,
                 transform=None, mode='train',
                 zip_mode=False, cache_mode=False, cache_db=False, ignore_db_cache=True,
                 tokenizer=None, pretrained_model_name=None,
                 add_image_as_a_box=False,
                 aspect_grouping=False, **kwargs):
        """
        VisualCometDataset Image Dataset
        """
        super(VisualCometDataset, self).__init__()

        self.seq_len = seq_len
        self.with_rel_task = with_rel_task
        self.with_mlm_task = with_mlm_task
        self.with_mvrc_task = with_mvrc_task
        self.with_contrastive_task = with_contrastive_task
        self.data_path = None
        self.root_path = root_path
        self.with_precomputed_visual_feat = with_precomputed_visual_feat
        self.mask_raw_pixels = mask_raw_pixels
        self.image_set = None
        self.mode = mode
        self.zip_mode = zip_mode
        self.cache_mode = cache_mode
        self.cache_db = cache_db
        self.ignore_db_cache = ignore_db_cache
        self.aspect_grouping = aspect_grouping
        self.add_image_as_a_box = add_image_as_a_box

        self.transform = transform
        self.cache_dir = os.path.join(self.root_path, 'cache')
        if not os.path.exists(self.cache_dir):
            makedirsExist(self.cache_dir)

        self.tokenizer = tokenizer if tokenizer is not None \
            else BertTokenizer.from_pretrained(
            'bert-large-uncased' if pretrained_model_name is None else pretrained_model_name,
            cache_dir=self.cache_dir)
        # self.tokenizer = tokenizer if tokenizer is not None \
        #     else VisualCometTokenizer.from_pretrained('gpt2', do_lower_case=True)

        self.record_img_event = []
        parse_record_cache = os.path.join(record_cache, '{}_record_img_event.txt'.format(self.mode))
        assert os.path.exists(
            parse_record_cache), "parse_record_cache needs to be existed, plese generate it first: call RecordParseDataset"

        with open(parse_record_cache, 'r+', encoding="utf-8") as f:
            for l in f.readlines():
                self.record_img_event.append(l.split(","))

        self.positive_pairs = None
        positive_pairs_cache = os.path.join(record_cache, 'dcbs_{}_positive_pairs.pkl'.format(self.mode))
        assert os.path.exists(
            positive_pairs_cache), "positive_pairs_cache needs to be existed, plese generate it first: call ImgPostivePairs"

        positive_pairs = torch.load(positive_pairs_cache)
        self.positive_pairs = positive_pairs['positive_pairs']

        with open(os.path.join(os.path.dirname(__file__), 'cocoontology.json'), 'r') as f:
            coco = json.load(f)
        self.coco_objects = ['__background__'] + [x['name'] for k, x in sorted(coco.items(), key=lambda x: int(x[0]))]
        self.coco_obj_to_ind = {o: i for i, o in enumerate(self.coco_objects)}


    @property
    def data_names(self):
        return ['image', 'boxes', 'im_info', 'text',
                'relationship_label', 'mlm_labels', 'mvrc_ops', 'mvrc_labels']

    def __getitem__(self, index):
        _p = random.random()
        if _p < 0.5 or (not self.with_rel_task):
            relationship_label = 1
        else:
            relationship_label = 0

        # if self.with_contrastive_task:
        return self.load_imgs(index, relationship_label) + self.load_imgs(self.positive_pairs[index], relationship_label)
        # else:
        #     return self.load_imgs(index, relationship_label)

    def load_imgs(self, index, relationship_label=1):
        img_fn, metadata_fn, caption = self.record_img_event[index]
        # event_tokens = self.tokenizer.basic_tokenizer.tokenize(event)
        caption_tokens = self.tokenizer.tokenize(caption)
        text_tokens = ['[CLS]'] + caption_tokens + ['[SEP]']
        text = self.tokenizer.convert_tokens_to_ids(text_tokens)

        # Load boxes and their features.
        with open(os.path.join(VCR_IMAGES_DIR, metadata_fn), 'r') as f:
            metadata = json.load(f)

        boxes = torch.as_tensor(np.array(metadata['boxes']))[:, :-1]

        objects = metadata['names']
        obj_labels = [self.coco_obj_to_ind[obj] for obj in objects]

        boxes_cls_scores = boxes.new_zeros((boxes.shape[0], 81))
        for i, class_ in enumerate(obj_labels):
            boxes_cls_scores[i, class_] = 1.0

        if self.with_precomputed_visual_feat:
            # Chop off the final dimension, that's the confidence
            id = img_fn[img_fn.rfind('/') + 1:img_fn.rfind('.')]
            with open(os.path.join(VCR_FEATURES_DIR, id) + '.pkl', 'rb') as p:
                features_dict = torch.load(p)
            img_features = features_dict['object_features']
        else:
            image = self._load_image(os.path.join(VCR_IMAGES_DIR, img_fn))
            w0, h0 = image.size

        if self.add_image_as_a_box:
            image_box = torch.as_tensor([[0.0, 0.0, w0 - 1.0, h0 - 1.0]])
            boxes = torch.cat((image_box, boxes), dim=0)
            obj_labels = [self.coco_obj_to_ind['__background__']] + obj_labels
            if self.with_precomputed_visual_feat:
                image_box_feat = boxes.mean(dim=0, keepdim=True)
                boxes = torch.cat((image_box_feat, boxes), dim=0)



        # transform
        im_info = torch.tensor([w0, h0, 1.0, 1.0, index])
        if self.transform is not None:
            image, boxes, _, im_info = self.transform(image, boxes, None, im_info)


        # clamp boxes
        w = im_info[0].item()
        h = im_info[1].item()
        boxes[:, [0, 2]] = boxes[:, [0, 2]].clamp(min=0, max=w - 1)
        boxes[:, [1, 3]] = boxes[:, [1, 3]].clamp(min=0, max=h - 1)

        relationship_label = relationship_label
        if relationship_label==0:
            rand_index = random.randrange(0, len(self.record_img_event))
            while rand_index == index:
                rand_index = random.randrange(0, len(self.record_img_event))
            caption = self.record_img_event[rand_index][2]

        # Task #2: Masked Language Modeling
        if self.with_mlm_task:
            caption_tokens = self.tokenizer.tokenize(caption)
            caption_tokens, mlm_labels = self.random_word_wwm(caption_tokens)
        else:
            caption_tokens = self.tokenizer.tokenize(caption)
            mlm_labels = [-1] * len(caption_tokens)
        text_tokens = ['[CLS]'] + caption_tokens + ['[SEP]']
        mlm_labels = [-1] + mlm_labels + [-1]

        # Task #3: Masked Visual Region Classification
        if self.with_mvrc_task:
            if self.add_image_as_a_box:
                mvrc_ops, mvrc_labels = self.random_mask_region(boxes_cls_scores)
                mvrc_ops = [0] + mvrc_ops
                mvrc_labels = [np.zeros_like(boxes_cls_scores[0])] + mvrc_labels
                num_real_boxes = boxes.shape[0] - 1
                num_masked_boxes = 0
                if self.with_precomputed_visual_feat:
                    assert False
                    # boxes_features[0] *= num_real_boxes
                    # for mvrc_op, box_feat in zip(mvrc_ops, boxes_features):
                    #     if mvrc_op == 1:
                    #         num_masked_boxes += 1
                    #         boxes_features[0] -= box_feat
                    # boxes_features[0] /= (num_real_boxes - num_masked_boxes + 1e-5)
            else:
                mvrc_ops, mvrc_labels = self.random_mask_region(boxes_cls_scores)
            assert len(mvrc_ops) == boxes.shape[0], \
                "Error: mvrc_ops have length {}, expected {}!".format(len(mvrc_ops), boxes.shape[0])
            assert len(mvrc_labels) == boxes.shape[0], \
                "Error: mvrc_labels have length {}, expected {}!".format(len(mvrc_labels), boxes.shape[0])
        else:
            mvrc_ops = [0] * boxes.shape[0]
            mvrc_labels = [np.zeros_like(boxes_cls_scores[0])] * boxes.shape[0]

        # zero out pixels of masked RoI
        if (not self.with_precomputed_visual_feat) and self.mask_raw_pixels:
            for mvrc_op, box in zip(mvrc_ops, boxes):
                if mvrc_op == 1:
                    x1, y1, x2, y2 = box
                    image[:, int(y1):(int(y2) + 1), int(x1):(int(x2) + 1)] = 0

        mvrc_labels = np.stack(mvrc_labels, axis=0)

        text = self.tokenizer.convert_tokens_to_ids(text_tokens)

        if self.with_precomputed_visual_feat:
            assert False
            # boxes = torch.cat((boxes, boxes_features), dim=1)

        # truncate seq to max len
        if len(text) + len(boxes) > self.seq_len:
            text_len_keep = len(text)
            box_len_keep = len(boxes)
            while (text_len_keep + box_len_keep) > self.seq_len:
                if box_len_keep > text_len_keep:
                    box_len_keep -= 1
                else:
                    text_len_keep -= 1
            boxes = boxes[:box_len_keep]
            text = text[:text_len_keep]
            mlm_labels = mlm_labels[:text_len_keep]
            mvrc_ops = mvrc_ops[:box_len_keep]
            mvrc_labels = mvrc_labels[:box_len_keep]

        if self.with_precomputed_visual_feat:
            image = img_features

        return image, boxes, im_info, text, relationship_label, mlm_labels, mvrc_ops, mvrc_labels

    def _to_boxes_and_masks(self, features, boxes, num_max_boxes=15):
        num_boxes = len(boxes)
        if num_boxes > num_max_boxes:
            return features[:num_max_boxes, :], boxes[:num_max_boxes, :], [1] * num_max_boxes
        d = len(features[0])
        padded_features = np.concatenate((features, np.zeros((num_max_boxes - num_boxes, d))))
        padded_boxes = np.concatenate((boxes, np.zeros((num_max_boxes - num_boxes, 4))))
        mask = np.concatenate((np.ones(num_boxes), np.zeros(num_max_boxes - num_boxes)), axis=0)

        return padded_features, padded_boxes, mask

    def __len__(self):
        return len(self.record_img_event)

    def _load_image(self, path):
        return Image.open(path).convert('RGB')

    def _load_json(self, path):
        with open(path, 'r') as f:
            return json.load(f)

    def random_word_wwm(self, tokens):
        output_tokens = []
        output_label = []

        for i, token in enumerate(tokens):
            sub_tokens = self.tokenizer.wordpiece_tokenizer.tokenize(token)
            prob = random.random()
            # mask token with 15% probability
            if prob < 0.15:
                prob /= 0.15

                # 80% randomly change token to mask token
                if prob < 0.8:
                    for sub_token in sub_tokens:
                        output_tokens.append("[MASK]")
                # 10% randomly change token to random token
                elif prob < 0.9:
                    for sub_token in sub_tokens:
                        output_tokens.append(random.choice(list(self.tokenizer.vocab.keys())))
                        # -> rest 10% randomly keep current token
                else:
                    for sub_token in sub_tokens:
                        output_tokens.append(sub_token)

                        # append current token to output (we will predict these later)
                for sub_token in sub_tokens:
                    try:
                        output_label.append(self.tokenizer.vocab[sub_token])
                    except KeyError:
                        # For unknown words (should not occur with BPE vocab)
                        output_label.append(self.tokenizer.vocab["[UNK]"])
                        logging.warning("Cannot find sub_token '{}' in vocab. Using [UNK] insetad".format(sub_token))
            else:
                for sub_token in sub_tokens:
                    # no masking token (will be ignored by loss function later)
                    output_tokens.append(sub_token)
                    output_label.append(-1)

        ## if no word masked, random choose a word to mask
        # if all([l_ == -1 for l_ in output_label]):
        #    choosed = random.randrange(0, len(output_label))
        #    output_label[choosed] = self.tokenizer.vocab[tokens[choosed]]

        return output_tokens, output_label

    def random_mask_region(self, regions_cls_scores):
        num_regions, num_classes = regions_cls_scores.shape
        output_op = []
        output_label = []
        for k, cls_scores in enumerate(regions_cls_scores):
            prob = random.random()
            # mask region with 15% probability
            if prob < 0.15:
                prob /= 0.15

                if prob < 0.9:
                    # 90% randomly replace appearance feature by "MASK"
                    output_op.append(1)
                else:
                    # -> rest 10% randomly keep current appearance feature
                    output_op.append(0)

                # append class of region to output (we will predict these later)
                output_label.append(cls_scores)
            else:
                # no masking region (will be ignored by loss function later)
                output_op.append(0)
                output_label.append(np.zeros_like(cls_scores))

        # # if no region masked, random choose a region to mask
        # if all([op == 0 for op in output_op]):
        #     choosed = random.randrange(0, len(output_op))
        #     output_op[choosed] = 1
        #     output_label[choosed] = regions_cls_scores[choosed]

        return output_op, output_label



if __name__ == '__main__':
    rpd = RecordParseDataset(load_cache=True)
    imgP = ImgPostivePairs(rpd, load_cache=True)
    print()

    # vcd = VisualCometDataset()
    # c_ll = BatchCollator(vcd)
    # data_loader = torch.utils.data.DataLoader(vcd, batch_size=8, collate_fn=c_ll)
    #
    # for i, data in enumerate(data_loader):
    #     print()
    #     break




